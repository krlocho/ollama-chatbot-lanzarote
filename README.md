# üåµ Ollama Chatbot Local

Este es un proyecto de chatbot web ligero que se ejecuta completamente en local. Utiliza **Ollama** como backend para la generaci√≥n de lenguaje y una interfaz de usuario simple construida con HTML, CSS (con tem√°tica de Lanzarote) y JavaScript.



---

## üöÄ Requisitos Previos

Para ejecutar este chatbot, necesitas tener instalados y funcionando los siguientes componentes:

1.  **Ollama:** El servidor de modelos de lenguaje local.
    * https://ollama.com
2.  **Un Modelo de Lenguaje:** Debes tener al menos un modelo descargado. Recomendamos `llama2` o `mistral` para empezar.

    ```bash
    # Ejemplo:
    ollama pull mistral
    ```
3.  **Un Servidor Web Local Simple:** Para evitar problemas de seguridad del navegador (CORS), el frontend debe servirse a trav√©s de HTTP (no abrir el archivo directamente).

---

## ‚öôÔ∏è Instalaci√≥n y Configuraci√≥n

Sigue estos pasos para poner en marcha el chatbot en tu m√°quina local.

### 1. Clonar el Repositorio
### 2. Iniciar el Backend (Ollama)

Aseg√∫rate de que el servicio de Ollama est√© ejecut√°ndose en segundo plano en http://localhost:11434.

Bash
# Si no est√° activo, ejecuta este comando:
ollama serve
### 3. Iniciar el Frontend (Servidor Web Local)

Para servir los archivos HTML, CSS y JS, inicia un servidor web simple desde la carpeta del proyecto (requiere Python 3):

Bash
python3 -m http.server 8000
### 4. Acceder al Chatbot

Una vez que ambos servidores est√©n activos, abre tu navegador y navega a la siguiente direcci√≥n:

http://localhost:8000/
### 5. Configurar Modelo

En la interfaz web:

El men√∫ desplegable "Modelo" se llenar√° autom√°ticamente con todos los modelos que tengas descargados de Ollama.

Selecciona el modelo (ej. mistral:latest) para iniciar la conversaci√≥n.

